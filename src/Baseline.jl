using StatsBase
using Catch22
import StatsBase.std
using LinearAlgebra

function reStandardise(F::AbstractArray)
    idxs = vec(nanrows(F) .| constantrows(F))
    ğ›” = StatsBase.std(F, dims=2)
    ğ› = mean(F, dims=2)
    ğ›”[idxs] .= Inf
    return X -> normalise(X, ğ›, ğ›”, standardise, 2)
end
export reStandardise

function reZero(F::AbstractArray, Î±::Float64=10.0)
    # We want to adjust a baseline data set so that features have a variance of 0 if the constant baseline is similar to the high dimensional baseline  (otherwise, the baseline does not change variance).
    # For this, a hyperbolic function of constant and high dimensional baseline variances is used to scale the output
    ğ›” = vec(StatsBase.std(F, dims=2))
    f(X) =  begin
                #C = 1 .- min.((1,), ğ›”./(vec(StatsBase.std(X, dims=2))))
                C = exp.(-Î±.*ğ›”./(vec(StatsBase.std(X, dims=2))))
                ğ› = vec(mean(X, dims=2))
                return normalise(X, ğ›, C, standardise, 2)
            end
    return f
end
export reZero


# ------------------------------------------------------------------------------------------------ #
#                            Combine a high and low dimensional baseline                           #
# ------------------------------------------------------------------------------------------------ #
function reScale(x::AbstractVector, f::Function=identity)
    Ïƒ = std(x)
    Ïƒâ€² = f(x)
    if Ïƒâ€² == Ïƒ == 0.0
        Ïƒâ€² = Ïƒ = 1.0 # Catch the limit
    end
    return Ïƒâ€².*x./Ïƒ
end
function reScale(F::AbstractArray, f::Vector)
    Fâ€² = deepcopy(F)
    for r = 1:size(F, 1)
        Fâ€²[r, :] = reScale(Fâ€²[r, :] , f[r])
    end
   return Fâ€²
end
function reScale(F::AbstractFeatureArray, f::AbstractFeatureVector)
    (Fáµ£, fáµ£) = intersectFeatures(F, f) # Assumes f has all of the features of F
    reScale(Fáµ£, vec(fáµ£))
end

function NonstationaryProcesses.rampInterval(Fâ‚—, Fâ‚•, F)
    ğ›”â‚—, ğ›”â‚• = std(Array(Fâ‚—), dims=2), std(Array(Fâ‚•), dims=2)
    ğ›”â‚•[ğ›”â‚• .< ğ›”â‚—] .= Inf
    ğŸ = [ğŸ -> rampInterval(0.0, 1.0, ğ›”â‚—[i], ğ›”â‚•[i])(std(ğŸ)) for i âˆˆ 1:size(F, 1)]
end

function intervalscale(Fâ‚—::Array{Float64, 2}, Fâ‚•::Array{Float64, 2},
                    interval::Function=rampInterval)
    if size(Fâ‚—, 1) != size(Fâ‚•, 1)
        error("High and low dimensional baselines do not have the same number of features")
    end
    return F -> reScale(F, interval(Fâ‚—, Fâ‚•, F))
end
function intervalscale(Fâ‚—::AbstractFeatureMatrix, Fâ‚•::AbstractFeatureMatrix,
    interval::Function=rampInterval)
    if any(Catch22.featureDims(Fâ‚—) .!= Catch22.featureDims(Fâ‚•))
        error("High and low dimensional baselines do not have the same features")
    end
    return F -> reScale(F,  Catch22.featureVector(interval(Fâ‚—, Fâ‚•, F), Catch22.featureDims(F)))
end
export intervalscale

# ------------------------------------------------------------------------------------------------ #
#                                    Scale a baseline using PCA                                    #
# ------------------------------------------------------------------------------------------------ #
function orthogonalise(F::AbstractArray, dimensionalityReduction=principalcomponents)
    M = dimensionalityReduction(F)
    FÌ‚ = embed(M, F)
    return (FÌ‚, M)
end
function orthogonalise(F::AbstractFeatureMatrix, dimensionalityReduction=principalcomponents)
    FÌ‚, M = orthogonalise(Array(F), dimensionalityReduction)
    FÌ‚ = Catch22.featureMatrix(FÌ‚, [Symbol("PC$x") for x âˆˆ 1:size(FÌ‚, 1)])
    return FÌ‚, M
end
export orthogonalise

function orthogonalBaseline(F::AbstractArray, dimensionalityReduction=principalcomponents)
    function ğ‘(F_test)
        @assert size(F, 1) == size(F_test, 1)
        FÌ‚, M = orthogonalise(F, dimensionalityReduction)
        F_out = embed(M, Array(F_test))
    end
    return ğ‘
end
function orthogonalBaseline(F::AbstractFeatureMatrix, dimensionalityReduction=principalcomponents)
    function ğ‘(F_test)
        FÌ‚_test, FÌ‚ = intersectFeatures(F_test, F)
        FÌ‚, M = orthogonalise(FÌ‚, dimensionalityReduction)
        F_out = embed(M, Array(FÌ‚_test))
        F_out = Catch22.featureMatrix(F_out, [Symbol("PC$x") for x âˆˆ 1:size(F_out, 1)])
    end
    return ğ‘
end
export orthogonalBaseline

function orthonormalHiloBaseline(F::AbstractFeatureArray, â„±â‚—::AbstractFeatureArray, â„±â‚•::AbstractFeatureArray; interval::Function=(x, y) -> NonstationaryProcesses.rampInterval(0, 1, x, y))
    F, â„±â‚—, â„±â‚• = intersectFeatures(F, â„±â‚—, â„±â‚•) # Intersects to the feature set of F
    â„±â‚•â€², M = orthogonalise(Array(â„±â‚•))
    â„±â€² = Catch22.featureMatrix(â„±â‚•â€², [Symbol("PC$x") for x âˆˆ 1:size(â„±â‚•â€², 1)])
    Fâ€² = embed(M, F)
    â„±â€²â‚— = embed(M, â„±â‚—)
    ğ‘â€² = intervalscale(Array(â„±â€²â‚—), Array(â„±â‚•â€²), interval)
    return ğ‘â€²(Fâ€²)
end
orthonormalHiloBaseline(â„±â‚—::AbstractFeatureArray, â„±â‚•::AbstractFeatureArray; kwargs...) = F -> orthonormalHiloBaseline(F, â„±â‚—, â„±â‚•; kwargs...)
export orthonormalHiloBaseline




# ------------------------------------------------------------------------------------------------ #
#                               Filter features using correlations/MI                              #
# ------------------------------------------------------------------------------------------------ #
function meanDependence(f, F; metric=StatsBase.corspearman)
    Ï = [metric(f, fâ€²) for fâ€² âˆˆ eachrow(F)]
    ÏÌ„ = mean(abs.(Ï))
end

function meanDependence(F; metric=StatsBase.corspearman)
    ğŸ = zeros(size(F, 1))
    ğŸ = [meanDependence(F[x, :], F[setdiff(1:size(F, 1), [x]), :]; metric) for x âˆˆ 1:size(F, 1)]
end
export meanDependence

function dependencyFilter(F, threshold=0.3; metric=StatsBase.corspearman, iterations=Inf, direction=:min)
    # Iteratively filter features according to algorithm in Ben's thesis, page 214
    iteration = 0
    if direction == :min
        direction = 1
    elseif direction == :max
        direction = -1
    end
    IÌ„ = zeros(size(F, 1)) .+ (-direction + 1)/2
    while any(direction.*IÌ„ .< direction.*threshold) && iteration < iterations # Yes IÌ„ keeps changing size
        IÌ„ = meanDependence(F; metric)
        I, f = findmin(direction.*IÌ„)
        F = F[setdiff(1:lastindex(F, 1), [f]), :]
        iteration += 1
    end
    return F
end
export dependencyFilter





# * We have four baselines associated with rescaling variances, plus the PCA whitening rotation that can be performed before all four rescalings

# ------------------------------------------- Rescaling ------------------------------------------ #
"""
Standardise the test features. Alternatively, just use the normalisation field of an Inference.
"""
standardbaseline(F::AbstractArray) = standardise(F, 2)
standardbaseline() = F -> standardbaseline(F)
export standardbaseline


"""
Scale the features so that the variance of a constant baseline is zero. Do this by setting mapping variances less than Ïƒâ‚— to 0, but keeping a gradient of 1.0 afterwards. Zscore features, zscore baseline from features and then map
"""
function lowbaseline(Fâ‚—::AbstractArray)
    interval = x -> NonstationaryProcesses.rampOn(0.0, 1.0, x, x+1.0)
    function lowscale(F::AbstractArray)
        FÌ‚, FÌ‚â‚— = intersectFeatures(F, Fâ‚—)
        ğ›” = StatsBase.std(FÌ‚, dims=2)
        ğ› = StatsBase.mean(FÌ‚, dims=2)
        FÌ‚â‚— = normalise(FÌ‚â‚—, ğ›, ğ›”, standardise, 2)
        FÌ‚ = normalise(FÌ‚, ğ›, ğ›”, standardise, 2)
        ğ›” = StatsBase.std(FÌ‚, dims=2)
        ğ› = StatsBase.mean(FÌ‚, dims=2)
        ğ›”â‚— = StatsBase.std(FÌ‚â‚—, dims=2)
        ğ›â‚— = StatsBase.mean(FÌ‚â‚—, dims=2)
        ğŸ = [x -> interval(Ïƒ)(StatsBase.std(x)) for Ïƒ âˆˆ ğ›”â‚—][:]
        if F isa AbstractFeatureArray
            ğŸ = Catch22.FeatureVector(ğŸ, Catch22.featureDims(FÌ‚â‚—))
        end
        return reScale(FÌ‚, ğŸ)
    end
    return lowscale
end
export lowbaseline

"""
Scale the features so that the variance of a high dimensional baselines is unity. Do this as an inverval with a constant variance of 0.0
"""
function highbaseline(Fâ‚•::AbstractArray)
    Fâ‚— = zeros(size(Fâ‚•))
    intervalscale(Fâ‚—, Fâ‚•)
end
function highbaseline(Fâ‚•::AbstractFeatureArray)
    Fâ‚•, Fâ‚— = intersectFeatures(Fâ‚•, zeros(size(Fâ‚•)))
    intervalscale(Fâ‚—, Fâ‚•)
end
export highbaseline

"""
Scale the features so that their variances map to a rampInterval between the low (0) and high dim (1) baselines.
"""
function intervalbaseline(Fâ‚—::AbstractArray, Fâ‚•::AbstractArray)
    intervalscale(Fâ‚—, Fâ‚•)
end
export intervalbaseline

# # ---------------------------------- High dim orthonormalisation --------------------------------- #
# """
# Add this to any baseline variables and the Inference normalisation to transform into the high dim. whitened space
# e.g. infer(S, var; parameters, features, baseline=intervalbaseline(ğ‘œ(Fâ‚—), ğ‘œ(Fâ‚•)), normalisation=ğ‘œ) # Note normalisation occurs before baseline
# """
# orthonormaliseto(Fâ‚•::AbstractArray, dimensionalityReduction=principalcomponents) = orthogonalBaseline(Fâ‚•, dimensionalityReduction)
# export orthonormaliseto


# ---------------------------------- High dim orthogonalisation --------------------------------- #
"""
Add this to a baseline after constructing a scaling baseline and using that as normalisation
e.g. ğ‘ = intervalbaseline(Fâ‚—, Fâ‚•)
infer(S, var; parameters, features, baseline=orthogonaliseto(ğ‘(Fâ‚•)), normalisation=ğ‘) # Note normalisation occurs before baseline
"""
orthogonaliseto(Fâ‚•::AbstractArray, dimensionalityReduction=principalcomponents) = orthogonalBaseline(Fâ‚•, dimensionalityReduction)
export orthogonaliseto



"""
Interval scaling informed by distributions. The idea is to decrease the scale of a feature if we are unsure of its location between the high and zero dim distributions
"""
function significanceinterval(Fâ‚—, Fâ‚•, F)
    ğŸ = rampInterval(Fâ‚—, Fâ‚•, F)
    ğ‘â‚€ = [f -> pvalue(VarianceFTest(Fâ‚—[i, :], f), tail=:right) for i âˆˆ 1:size(Fâ‚—, 1)]
    ğ‘â‚• = [f -> pvalue(VarianceFTest(Fâ‚•[i, :], f), tail=:left) for i âˆˆ 1:size(Fâ‚•, 1)]
    function out(f, i)
        ğŸ[i](f[:]).*ğ‘â‚€[i](f[:]).*ğ‘â‚•[i](f[:])
    end
    return [f -> out(f, i) for i âˆˆ 1:size(ğŸ, 1)]
end
export significanceinterval


"""
errorintervalscaling
"""
function errorintervalscaling(Fâ‚—, Fâ‚•, F)
    ğŸ = rampInterval(Fâ‚—, Fâ‚•, F)
    function out(f, i)
        Î” = rampInterval(0.0, 1.0, 0.0, 1.0)(bootstrapSEÏƒ(f[:])/(std(Fâ‚•[i, :]) - std(Fâ‚—[i, :]))) # So it saturates at extremes
        ğŸ[i](f[:])*(1 - Î”)
    end
    return [f -> out(f, i) for i âˆˆ 1:size(ğŸ, 1)]
end
export errorintervalscaling


"""
Scale the features so that their variances map to a rampInterval between the low (0) and high dim (1) baselines.
"""
function errorintervalbaseline(Fâ‚—::AbstractArray, Fâ‚•::AbstractArray)
    intervalscale(Fâ‚—, Fâ‚•, errorintervalscaling)
end
export errorintervalbaseline




"""
What's this?
"""
# Total covariance correction
function dependencyscalingnorotation(ğ‘, Fâ‚•)
    Fâ‚•â€² = ğ‘(Fâ‚•)
    function g(F)
        Fâ€², Fâ‚•â€² = intersectFeatures(noconstantrows(F), Fâ‚•â€²)
        # Fâ‚•â€², Fâ€² = intersectFeatures(noconstantrows(Fâ‚•â€²), Fâ€²)
        Î£â‚•Â² = StatsBase.cov(Array(Fâ‚•â€²), dims=2)
        ğ‘œ = orthogonaliseto(Fâ‚•â€², principalcomponents)
        ğ§ = sum(abs.(Array(Î£â‚•Â²)), dims=2)
        ğ§[ğ§ .== 0] .= Inf
        Nâ»Â¹ = FeatureMatrix(inv(sqrt(Diagonal(ğ§[:]))), getnames(Fâ‚•â€²))
        return FeatureMatrix(Nâ»Â¹*ğ‘(F), getnames(Fâ€²))
    end
    return g
end
export dependencyscalingnorotation
